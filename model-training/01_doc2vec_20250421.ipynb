{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b14018b",
   "metadata": {},
   "source": [
    "## Training Doc2Vec on German Political Speeches\n",
    "\n",
    "This notebook trains a Doc2Vec model on 1912 transcribed videos from the official channels of both the far right-wing party AfD (Alternative f√ºr Deutschland) in Germany and the left-wing party Die Linke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ab99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependincies\n",
    "%pip install gensim\n",
    "pip install pypet\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030d43d",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.doc2vec as d2v\n",
    "import nltk\n",
    "from pypet import progressbar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773c025",
   "metadata": {},
   "source": [
    "Loading my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9530de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETPATH = \"data/combined_dataset.json\"\n",
    "\n",
    "with open(DATASETPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f6c75",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddf70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    item.pop(\"score\", None)\n",
    "    item.pop(\"title\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd14047",
   "metadata": {},
   "source": [
    "Taging each speech with a unique ID regardless of the party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bcbb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = {}\n",
    "for idx, rec in enumerate(data):\n",
    "    speech_id = f\"speech_{idx+1}\" \n",
    "    speeches[speech_id] = {\n",
    "        \"party\": rec[\"party\"],\n",
    "        \"speech\": rec[\"transcript\"],\n",
    "    }\n",
    "\n",
    "print(f\"Number of speeches: {len(speeches)}\")\n",
    "print(list(speeches.items())[0])\n",
    "print(speeches[\"speech_1\"][\"speech\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e8ad9",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "NLTK will be applyed to tokenize the documents. <br>\n",
    "For this step the tokenization will be purely applied and using space as delimiter and only lowering the cases.\n",
    "<br> For later and another way to vectorize we can use:  <br> <br>\n",
    "`gensim.utils.simple_preprocess(doc, deacc=False, min_len=2, max_len=15)`\n",
    "[doc2vec documentation](https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess) <br>\n",
    "Additionaly, I would want to create stopwords list and remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_speechs(speechs):\n",
    "    tokens = {}\n",
    "    for idx, speech_id in enumerate(speechs):\n",
    "        speech = speeches[speech_id]['speech']\n",
    "        tokenized = [x.lower() for x in nltk.word_tokenize(speech, language='german')]\n",
    "        tokens[speech_id] = tokenized\n",
    "        progressbar(idx, len(speechs), reprint=False)\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_speechs(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ddb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing the first token to check it's structure\n",
    "print(list(tokens.items())[0])\n",
    "print(list(tokens.items())[0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51429cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using doc2vec to created tagged documents\n",
    "def create_tagged_objects(tokens):\n",
    "    \"\"\"Converts tokens to gensim tagged documents\"\"\"\n",
    "    tagged_docs = {}\n",
    "    for idx, com_id in enumerate(tokens):\n",
    "        tagged_doc = d2v.TaggedDocument(words=tokens[com_id], tags=[com_id])\n",
    "        tagged_docs[com_id]= tagged_doc\n",
    "        progressbar(idx, len(speeches), percentage_step=5, reprint=False)\n",
    "    return tagged_docs\n",
    "\n",
    "tagged_docs = create_tagged_objects(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(tagged_docs.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test/test 80/20\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=[item[\"party\"] for item in data], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6fe70",
   "metadata": {},
   "source": [
    "Converting to pandas dataframe and renaming party to lables to avoid model confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ab7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_df)\n",
    "\n",
    "if 'labels' not in train_df.columns and 'party' in train_df.columns:\n",
    "    train_df.rename(columns={'party': 'labels'}, inplace=True)\n",
    "\n",
    "print(train_df[\"labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b54697",
   "metadata": {},
   "source": [
    "Tagging the documents with unique id for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tagged_docs = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    speech_id = f\"train_{idx}\"\n",
    "    tokenized = [x.lower() for x in nltk.word_tokenize(row[\"transcript\"], language='german')]\n",
    "    doc = d2v.TaggedDocument(words=tokenized, tags=[speech_id])\n",
    "    training_tagged_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f20425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the tages \n",
    "training_tagged_docs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc568d",
   "metadata": {},
   "source": [
    "Training doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = d2v.Doc2Vec(vector_size=256,  \n",
    "                    window=8, \n",
    "                    min_count=5, \n",
    "                    workers=4,  \n",
    "                    sample=1e-4,\n",
    "                    negative=5,\n",
    "                    alpha=0.05, \n",
    "                    min_alpha=0.001)  \n",
    "\n",
    "model.build_vocab(training_tagged_docs)\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Update learning rate for this epoch\n",
    "    alpha = 0.05 - (0.05 - 0.001) * (epoch / epochs)\n",
    "    model.alpha = alpha\n",
    "    model.min_alpha = alpha\n",
    "    \n",
    "    # Train for one epoch\n",
    "    model.train(training_tagged_docs, \n",
    "                total_examples=model.corpus_count, \n",
    "                epochs=1)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Alpha: {alpha:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49174d2",
   "metadata": {},
   "source": [
    "Checking if the model is able to identify the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geting a speech and seeing it\n",
    "print(train_df.loc[800])\n",
    "training_tagged_docs[800]\n",
    "\n",
    "# Usind the model to get the most_similar to index 800 in the traing dataset\n",
    "print(\"\\n Top 3 simillar\")\n",
    "doc_vec = model.docvecs[800]\n",
    "model.docvecs.most_similar([doc_vec], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7123d91f",
   "metadata": {},
   "source": [
    "`infere_vector()` is doc2vec function that retrun the vector represenation of a post training new document.<br>\n",
    "Also when runing on the same document, each time it returns a different representations of the same document. <br>\n",
    "<br>\n",
    "For more stability increase the number of epochs to have more control. \n",
    "[Doc2Vec documentation](https://radimrehurek.com/gensim/models/doc2vec.html?utm_source=chatgpt.com#gensim.models.doc2vec.Doc2Vec.infer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#infere_vector computes the vec of new input text\n",
    "def infer_vector(model, text):\n",
    "    \"\"\"Infer vector for a new piece of text\"\"\"\n",
    "    tokenized = [x.lower() for x in nltk.word_tokenize(text, language='german')]\n",
    "    return model.infer_vector(tokenized, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af71778",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Berlin, 17. April 2025. Zu den Pl√§nen der amtierenden Bundesregierung im Rahmen des sogenannten ‚ÄûResettlement‚Äú-Programms nun auch Menschen aus dem Sudan per Flugzeug diskret nach Deutschland zu holen teilt die AfD-Bundessprecherin Alice Weidel mit: ‚ÄûObwohl die Belastungsgrenze l√§ngst √ºberschritten ist, die Massenmigration unsere sozialen Sicherungssysteme √ºberfordert und die innere Sicherheit zusehends erodiert, hat die gescheiterte Rest-Ampel auf ihren letzten Metern nichts Besseres zu tun als noch m√∂glichst viele weitere Migranten nach Deutschland zu verbringen. Neben den Maschinen mit Afghanen fliegt die Regierung nun auch noch Sudanesen ein ‚Äì m√∂glichst ger√§uschlos, ohne jede √∂ffentliche Debatte und gegen den erkl√§rten Willen der Bev√∂lkerungsmehrheit. 2025 sollen so insgesamt 6.560 Migranten zus√§tzlich in Deutschland ‚Äöangesiedelt‚Äò werden. Ein skandal√∂ser Vorgang sondergleichen. Diese Art weltfremder und ideologiegetriebener Politik ist nicht nur verantwortungslos, sondern brandgef√§hrlich f√ºr den sozialen Frieden in unserem Land. Und trotz der gro√üspurigen Ank√ºndigungspolitik von schwarz-rot, derartige Programme zu beenden, geht das UN-Fl√ºchtlingswerk in Deutschland bereits davon aus, dass auch die neue Bundesregierung das ‚ÄöResettlement‚Äò weiterf√ºhren wird. Die AfD fordert die Regierung auf, alle M√∂glichkeiten aussch√∂pfen, jeden weiteren Massenzustrom nach Deutschland zu unterbinden. Wir fordern ein sofortiges Ende s√§mtlicher Bundesaufnahmeprogramme ‚Äì Deutschland ist kein Siedlungsgebiet. Wir fordern die unverz√ºgliche Einf√ºhrung von wirklich effektivem Grenzschutz, das hei√üt mit konsequenter Abweisung Illegaler. CDU und CSU haben eine echte Migrationswende versprochen und auch zur Bedingung f√ºr eine Regierungsbeteiligung erhoben. Sollte Merz nicht Wort halten, hat die Union nach ihrem Totalversagen 2015 in der Migrations- und Sicherheitspolitik nun endg√ºltig jegliche Glaubw√ºrdigkeit verloren.‚Äú\"\n",
    "sample_vec = infer_vector(model, sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs = model.docvecs.most_similar([sample_vec], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, similarity in similar_docs:\n",
    "    id = int(doc_id.split(\"_\")[1])\n",
    "    print(f\"{train_df.loc[id][\"labels\"]}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b99f2",
   "metadata": {},
   "source": [
    "### Stage 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# histogram of length of transcripts (number of words, number of tokens)\n",
    "transcript_lengths = []  \n",
    "ids = []\n",
    "\n",
    "for idex, item in enumerate(data):\n",
    "    trans_length = len(item[\"transcript\"])\n",
    "    transcript_lengths.append(trans_length)\n",
    "    ids.append(idex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54aaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(ids, transcript_lengths, width=2.8)\n",
    "plt.xlabel('Speech ID')\n",
    "plt.ylabel('Length of Transcript')\n",
    "plt.show()\n",
    "plt.hist(transcript_lengths, bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157973fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_docs_2_scores(similar_docs):\n",
    "  scores = {}\n",
    "  for doc_id, similarity in similar_docs:\n",
    "    num_id = int(doc_id.split(\"_\")[1])\n",
    "    label = train_df.loc[num_id][\"labels\"]\n",
    "    scores[label] = scores.get(label, 0) + similarity\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs_2_scores(similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_df)\n",
    "\n",
    "if 'labels' not in test_df.columns and 'party' in test_df.columns:\n",
    "    test_df.rename(columns={'party': 'labels'}, inplace=True)\n",
    "\n",
    "print(test_df[\"labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df[\"labels\"]\n",
    "print(test_labels[1])\n",
    "test_transcript = test_df[\"transcript\"]\n",
    "print(test_transcript[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27add983",
   "metadata": {},
   "source": [
    "Creating Predictions on the test dataset <br> \n",
    "1. Creating an empty test_prediction list. To use later for accuracy. \n",
    "2. Loops through the test trascripts\n",
    "3. Gets top 10 similar vectors \n",
    "4. applys ``similar_docs_2_scores()`` to get the highst score accorss top 10 similar\n",
    "5. Gets the party with the highst value \n",
    "5. Appends party lable to ``test_prediction``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = []\n",
    "\n",
    "for transcript in test_transcript:\n",
    "  sample_vec = infer_vector(model, transcript)\n",
    "  similar_docs = model.docvecs.most_similar([sample_vec], topn=10)\n",
    "\n",
    "  doc_scores = similar_docs_2_scores(similar_docs)\n",
    "  print(\"Document Score: \",doc_scores)\n",
    "  \n",
    "  # get the key with the highest values\n",
    "  doc_scores_list = [(k,v) for k,v in doc_scores.items()] # [(\"afd\", 0.543), (\"linke\", 0.123)]\n",
    "  print(\"Document score list: \",doc_scores_list)\n",
    "  doc_highest_score = max(doc_scores_list, key=lambda x: x[1]) # (\"afd\", 0.543)\n",
    "  print(\"Docuemtn Highst score\",doc_highest_score)\n",
    "\n",
    "  test_prediction.append(doc_highest_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_prediction[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c360913",
   "metadata": {},
   "source": [
    "Confusion Matrix from Sklearn to check the true values vs. the predicted values from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b87b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have test_predictions and test_labels\n",
    "# measure accuracy, recall, precision, look at confusion matrix\n",
    "#   might have to encode test_predictions and test_labels\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_prediction, labels=[\"Die Linke\", \"AFD\"])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Die Linke\", \"AFD\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, test_prediction, target_names=[\"Die Linke\", \"AFD\"]))\n",
    "accuracy = ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
