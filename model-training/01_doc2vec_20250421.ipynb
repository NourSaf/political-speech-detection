{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b14018b",
   "metadata": {},
   "source": [
    "## Training Doc2Vec on German Political Speeches\n",
    "\n",
    "This notebook trains a Doc2Vec model on 1912 transcribed videos from the official channels of both the far right-wing party AfD (Alternative für Deutschland) in Germany and the left-wing party Die Linke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ab99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependincies\n",
    "%pip install gensim\n",
    "pip install pypet\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030d43d",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.doc2vec as d2v\n",
    "import nltk\n",
    "from pypet import progressbar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773c025",
   "metadata": {},
   "source": [
    "Loading my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9530de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETPATH = \"data/combined_dataset.json\"\n",
    "\n",
    "with open(DATASETPATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f6c75",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddf70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    item.pop(\"score\", None)\n",
    "    item.pop(\"title\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd14047",
   "metadata": {},
   "source": [
    "Taging each speech with a unique ID regardless of the party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bcbb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = {}\n",
    "for idx, rec in enumerate(data):\n",
    "    speech_id = f\"speech_{idx+1}\" \n",
    "    speeches[speech_id] = {\n",
    "        \"party\": rec[\"party\"],\n",
    "        \"speech\": rec[\"transcript\"],\n",
    "    }\n",
    "\n",
    "print(f\"Number of speeches: {len(speeches)}\")\n",
    "print(list(speeches.items())[0])\n",
    "print(speeches[\"speech_1\"][\"speech\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e8ad9",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "NLTK will be applyed to tokenize the documents. <br>\n",
    "For this step the tokenization will be purely applied and using space as delimiter and only lowering the cases.\n",
    "<br> For later and another way to vectorize we can use:  <br> <br>\n",
    "`gensim.utils.simple_preprocess(doc, deacc=False, min_len=2, max_len=15)`\n",
    "[doc2vec documentation](https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess) <br>\n",
    "Additionaly, I would want to create stopwords list and remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_speechs(speechs):\n",
    "    tokens = {}\n",
    "    for idx, speech_id in enumerate(speechs):\n",
    "        speech = speeches[speech_id]['speech']\n",
    "        tokenized = [x.lower() for x in nltk.word_tokenize(speech, language='german')]\n",
    "        tokens[speech_id] = tokenized\n",
    "        progressbar(idx, len(speechs), reprint=False)\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_speechs(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ddb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing the first token to check it's structure\n",
    "print(list(tokens.items())[0])\n",
    "print(list(tokens.items())[0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51429cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using doc2vec to created tagged documents\n",
    "def create_tagged_objects(tokens):\n",
    "    \"\"\"Converts tokens to gensim tagged documents\"\"\"\n",
    "    tagged_docs = {}\n",
    "    for idx, com_id in enumerate(tokens):\n",
    "        tagged_doc = d2v.TaggedDocument(words=tokens[com_id], tags=[com_id])\n",
    "        tagged_docs[com_id]= tagged_doc\n",
    "        progressbar(idx, len(speeches), percentage_step=5, reprint=False)\n",
    "    return tagged_docs\n",
    "\n",
    "tagged_docs = create_tagged_objects(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(tagged_docs.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test/test 80/20\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=[item[\"party\"] for item in data], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6fe70",
   "metadata": {},
   "source": [
    "Converting to pandas dataframe and renaming party to lables to avoid model confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ab7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_df)\n",
    "\n",
    "if 'labels' not in train_df.columns and 'party' in train_df.columns:\n",
    "    train_df.rename(columns={'party': 'labels'}, inplace=True)\n",
    "\n",
    "print(train_df[\"labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b54697",
   "metadata": {},
   "source": [
    "Tagging the documents with unique id for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tagged_docs = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    speech_id = f\"train_{idx}\"\n",
    "    tokenized = [x.lower() for x in nltk.word_tokenize(row[\"transcript\"], language='german')]\n",
    "    doc = d2v.TaggedDocument(words=tokenized, tags=[speech_id])\n",
    "    training_tagged_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f20425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the tages \n",
    "training_tagged_docs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc568d",
   "metadata": {},
   "source": [
    "Training doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = d2v.Doc2Vec(vector_size=256,  \n",
    "                    window=8, \n",
    "                    min_count=5, \n",
    "                    workers=4,  \n",
    "                    sample=1e-4,\n",
    "                    negative=5,\n",
    "                    alpha=0.05, \n",
    "                    min_alpha=0.001)  \n",
    "\n",
    "model.build_vocab(training_tagged_docs)\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Update learning rate for this epoch\n",
    "    alpha = 0.05 - (0.05 - 0.001) * (epoch / epochs)\n",
    "    model.alpha = alpha\n",
    "    model.min_alpha = alpha\n",
    "    \n",
    "    # Train for one epoch\n",
    "    model.train(training_tagged_docs, \n",
    "                total_examples=model.corpus_count, \n",
    "                epochs=1)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Alpha: {alpha:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49174d2",
   "metadata": {},
   "source": [
    "Checking if the model is able to identify the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geting a speech and seeing it\n",
    "print(train_df.loc[800])\n",
    "training_tagged_docs[800]\n",
    "\n",
    "# Usind the model to get the most_similar to index 800 in the traing dataset\n",
    "print(\"\\n Top 3 simillar\")\n",
    "doc_vec = model.docvecs[800]\n",
    "model.docvecs.most_similar([doc_vec], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7123d91f",
   "metadata": {},
   "source": [
    "`infere_vector()` is doc2vec function that retrun the vector represenation of a post training new document.<br>\n",
    "Also when runing on the same document, each time it returns a different representations of the same document. <br>\n",
    "<br>\n",
    "For more stability increase the number of epochs to have more control. \n",
    "[Doc2Vec documentation](https://radimrehurek.com/gensim/models/doc2vec.html?utm_source=chatgpt.com#gensim.models.doc2vec.Doc2Vec.infer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#infere_vector computes the vec of new input text\n",
    "def infer_vector(model, text):\n",
    "    \"\"\"Infer vector for a new piece of text\"\"\"\n",
    "    tokenized = [x.lower() for x in nltk.word_tokenize(text, language='german')]\n",
    "    return model.infer_vector(tokenized, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af71778",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Berlin, 17. April 2025. Zu den Plänen der amtierenden Bundesregierung im Rahmen des sogenannten „Resettlement“-Programms nun auch Menschen aus dem Sudan per Flugzeug diskret nach Deutschland zu holen teilt die AfD-Bundessprecherin Alice Weidel mit: „Obwohl die Belastungsgrenze längst überschritten ist, die Massenmigration unsere sozialen Sicherungssysteme überfordert und die innere Sicherheit zusehends erodiert, hat die gescheiterte Rest-Ampel auf ihren letzten Metern nichts Besseres zu tun als noch möglichst viele weitere Migranten nach Deutschland zu verbringen. Neben den Maschinen mit Afghanen fliegt die Regierung nun auch noch Sudanesen ein – möglichst geräuschlos, ohne jede öffentliche Debatte und gegen den erklärten Willen der Bevölkerungsmehrheit. 2025 sollen so insgesamt 6.560 Migranten zusätzlich in Deutschland ‚angesiedelt‘ werden. Ein skandalöser Vorgang sondergleichen. Diese Art weltfremder und ideologiegetriebener Politik ist nicht nur verantwortungslos, sondern brandgefährlich für den sozialen Frieden in unserem Land. Und trotz der großspurigen Ankündigungspolitik von schwarz-rot, derartige Programme zu beenden, geht das UN-Flüchtlingswerk in Deutschland bereits davon aus, dass auch die neue Bundesregierung das ‚Resettlement‘ weiterführen wird. Die AfD fordert die Regierung auf, alle Möglichkeiten ausschöpfen, jeden weiteren Massenzustrom nach Deutschland zu unterbinden. Wir fordern ein sofortiges Ende sämtlicher Bundesaufnahmeprogramme – Deutschland ist kein Siedlungsgebiet. Wir fordern die unverzügliche Einführung von wirklich effektivem Grenzschutz, das heißt mit konsequenter Abweisung Illegaler. CDU und CSU haben eine echte Migrationswende versprochen und auch zur Bedingung für eine Regierungsbeteiligung erhoben. Sollte Merz nicht Wort halten, hat die Union nach ihrem Totalversagen 2015 in der Migrations- und Sicherheitspolitik nun endgültig jegliche Glaubwürdigkeit verloren.“\"\n",
    "sample_vec = infer_vector(model, sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs = model.docvecs.most_similar([sample_vec], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, similarity in similar_docs:\n",
    "    id = int(doc_id.split(\"_\")[1])\n",
    "    print(f\"{train_df.loc[id][\"labels\"]}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b99f2",
   "metadata": {},
   "source": [
    "### Stage 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# histogram of length of transcripts (number of words, number of tokens)\n",
    "transcript_lengths = []  \n",
    "ids = []\n",
    "\n",
    "for idex, item in enumerate(data):\n",
    "    trans_length = len(item[\"transcript\"])\n",
    "    transcript_lengths.append(trans_length)\n",
    "    ids.append(idex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54aaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(ids, transcript_lengths, width=2.8)\n",
    "plt.xlabel('Speech ID')\n",
    "plt.ylabel('Length of Transcript')\n",
    "plt.show()\n",
    "plt.hist(transcript_lengths, bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157973fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_docs_2_scores(similar_docs):\n",
    "  scores = {}\n",
    "  for doc_id, similarity in similar_docs:\n",
    "    num_id = int(doc_id.split(\"_\")[1])\n",
    "    label = train_df.loc[num_id][\"labels\"]\n",
    "    scores[label] = scores.get(label, 0) + similarity\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs_2_scores(similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_df)\n",
    "\n",
    "if 'labels' not in test_df.columns and 'party' in test_df.columns:\n",
    "    test_df.rename(columns={'party': 'labels'}, inplace=True)\n",
    "\n",
    "print(test_df[\"labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df[\"labels\"]\n",
    "print(test_labels[1])\n",
    "test_transcript = test_df[\"transcript\"]\n",
    "print(test_transcript[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27add983",
   "metadata": {},
   "source": [
    "Creating Predictions on the test dataset <br> \n",
    "1. Creating an empty test_prediction list. To use later for accuracy. \n",
    "2. Loops through the test trascripts\n",
    "3. Gets top 10 similar vectors \n",
    "4. applys ``similar_docs_2_scores()`` to get the highst score accorss top 10 similar\n",
    "5. Gets the party with the highst value \n",
    "5. Appends party lable to ``test_prediction``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = []\n",
    "\n",
    "for transcript in test_transcript:\n",
    "  sample_vec = infer_vector(model, transcript)\n",
    "  similar_docs = model.docvecs.most_similar([sample_vec], topn=10)\n",
    "\n",
    "  doc_scores = similar_docs_2_scores(similar_docs)\n",
    "  print(\"Document Score: \",doc_scores)\n",
    "  \n",
    "  # get the key with the highest values\n",
    "  doc_scores_list = [(k,v) for k,v in doc_scores.items()] # [(\"afd\", 0.543), (\"linke\", 0.123)]\n",
    "  print(\"Document score list: \",doc_scores_list)\n",
    "  doc_highest_score = max(doc_scores_list, key=lambda x: x[1]) # (\"afd\", 0.543)\n",
    "  print(\"Docuemtn Highst score\",doc_highest_score)\n",
    "\n",
    "  test_prediction.append(doc_highest_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_prediction[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c360913",
   "metadata": {},
   "source": [
    "Confusion Matrix from Sklearn to check the true values vs. the predicted values from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b87b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have test_predictions and test_labels\n",
    "# measure accuracy, recall, precision, look at confusion matrix\n",
    "#   might have to encode test_predictions and test_labels\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_prediction, labels=[\"Die Linke\", \"AFD\"])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Die Linke\", \"AFD\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, test_prediction, target_names=[\"Die Linke\", \"AFD\"]))\n",
    "accuracy = ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
